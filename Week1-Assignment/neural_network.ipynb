{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making a Feedforward Neural Network from scratch (Optional)\n",
    "We will try to make a feedforward neural network from scratch. This is an optional assignment. If you are even a little bit familiar with neural networks, know what neurons, weights, biases, feed-forward network, back propagration, gradient descent, activation functions are, then you can start right away. If not, don't worry. You can take a look at this medium article: https://medium.com/@waadlingaadil/learn-to-build-a-neural-network-from-scratch-yes-really-cac4ca457efc and try to learn what all these terms/topics are. Just knowing what they are won't help. I would recommend you to do the math along with the reading (because math is awesome!). Also avoid using for loops in this whole assignment. Now let's start!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import the most important library required for machine learning (Tensorflow? Pytorch? No. We are not going to use premade models, so these libraries are not required). Guess the short form for the library too :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import the library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the neural network is for some dataset, right? We will use the MNIST dataset. It consists of 60k training images and 10k test images of size 28 $\\times$ 28 with some digit written on each image. Our goal is to make a neural network which can identify the digit written on the image. Loading the dataset is done for you in the next block. Do not change anything in the block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DO NOT MODIFY THIS BLOCK\n",
    "### This block has the class for loading the MNIST dataset\n",
    "\n",
    "import struct\n",
    "from array import array\n",
    "from os.path import join\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "class MnistDataloader(object):\n",
    "    def __init__(self, training_images_filepath,training_labels_filepath,\n",
    "                 test_images_filepath, test_labels_filepath):\n",
    "        self.training_images_filepath = training_images_filepath\n",
    "        self.training_labels_filepath = training_labels_filepath\n",
    "        self.test_images_filepath = test_images_filepath\n",
    "        self.test_labels_filepath = test_labels_filepath\n",
    "    \n",
    "    def read_images_labels(self, images_filepath, labels_filepath):        \n",
    "        labels = []\n",
    "        with open(labels_filepath, 'rb') as file:\n",
    "            magic, size = struct.unpack(\">II\", file.read(8))\n",
    "            if magic != 2049:\n",
    "                raise ValueError('Magic number mismatch, expected 2049, got {}'.format(magic))\n",
    "            labels = array(\"B\", file.read())        \n",
    "        \n",
    "        with open(images_filepath, 'rb') as file:\n",
    "            magic, size, rows, cols = struct.unpack(\">IIII\", file.read(16))\n",
    "            if magic != 2051:\n",
    "                raise ValueError('Magic number mismatch, expected 2051, got {}'.format(magic))\n",
    "            image_data = array(\"B\", file.read())        \n",
    "        images = []\n",
    "        for i in range(size):\n",
    "            images.append([0] * rows * cols)\n",
    "        for i in range(size):\n",
    "            img = np.array(image_data[i * rows * cols:(i + 1) * rows * cols])\n",
    "            img = img.reshape(28, 28)\n",
    "            images[i][:] = img            \n",
    "        \n",
    "        return images, labels\n",
    "            \n",
    "    def load_data(self):\n",
    "        x_train, y_train = self.read_images_labels(self.training_images_filepath, self.training_labels_filepath)\n",
    "        x_test, y_test = self.read_images_labels(self.test_images_filepath, self.test_labels_filepath)\n",
    "        return (x_train, y_train),(x_test, y_test)\n",
    "    \n",
    "def show_images(images, title_texts):\n",
    "    cols = 3\n",
    "    rows = int(len(images)/cols) + 1\n",
    "    plt.figure(figsize=(20,20))\n",
    "    index = 1    \n",
    "    for x in zip(images, title_texts):        \n",
    "        image = x[0]        \n",
    "        title_text = x[1]\n",
    "        plt.subplot(rows, cols, index)        \n",
    "        plt.imshow(image, cmap=plt.cm.gray)\n",
    "        if (title_text != ''):\n",
    "            plt.title(title_text, fontsize = 15);        \n",
    "        index += 1\n",
    "\n",
    "training_images_filepath = \"mnist_dataset/train-images.idx3-ubyte\"\n",
    "training_labels_filepath = \"mnist_dataset/train-labels.idx1-ubyte\"\n",
    "test_images_filepath = \"mnist_dataset/t10k-images.idx3-ubyte\"\n",
    "test_labels_filepath = \"mnist_dataset/t10k-labels.idx1-ubyte\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DO NOT MODIFY THIS BLOCK\n",
    "## This block loads the MNIST dataset\n",
    "## Just run this block once\n",
    "\n",
    "mnist_dataloader = MnistDataloader(training_images_filepath, training_labels_filepath, test_images_filepath, test_labels_filepath)\n",
    "(x_train, y_train), (x_test, y_test) = mnist_dataloader.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x_train and x_test have the images in list form, and y_train and y_test have the labels of the images stored as a list. You can try printing the values in these variables to see what shape they have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK: find out the shapes of all these lists and print them\n",
    "# TODO\n",
    "\n",
    "# END TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## You always need to check how your data looks like, right?\n",
    "## Run this block to see the some random 3 training and test images\n",
    "## Change the variable number_of_images_to_show as per your requirement\n",
    "\n",
    "images_2_show = []\n",
    "titles_2_show = []\n",
    "number_of_images_to_show = 3\n",
    "# 3 random training images\n",
    "for i in range(0, number_of_images_to_show):\n",
    "    r = random.randint(1, 60000)\n",
    "    images_2_show.append(x_train[r])\n",
    "    titles_2_show.append('training image [' + str(r) + '] = ' + str(y_train[r]))    \n",
    "\n",
    "# 3 random test images\n",
    "for i in range(0, number_of_images_to_show):\n",
    "    r = random.randint(1, 10000)\n",
    "    images_2_show.append(x_test[r])        \n",
    "    titles_2_show.append('test image [' + str(r) + '] = ' + str(y_test[r]))    \n",
    "\n",
    "show_images(images_2_show, titles_2_show)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "Data preprocessing is a very important step in data analysis. It includes cleaning, integration, transformation, reduction, discretization and normalization of the given dataset. Outlier analysis is also an important step in preprocessing. Here, the data is clean (our work has been reduced).<br><br>\n",
    "We will only scale the data. If you try printing the images e.g. <code>print(x_train[0])</code>, you will find that the values in the list lie between 0 to 255. Scale these down to 0 to 1. Also convert the images x_train, x_test from list to numpy array. Also flatten this array to a shape (x_train.shape[0], __, 1) to make each image in it compatible with the input layer of our neural network.\n",
    "\n",
    "Realize that after flattening the image, it went from size of 28x28(2 dimensional) to a size of 784 (1 dimensional). We flatten the image so that we can feed these pixel values into the feed forward neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x_train, x_test):\n",
    "\t# <START>\n",
    "\t# Convert the data to numpy arrays\n",
    "\n",
    "\t# Scale the data to the range of 0-1\n",
    "\t\n",
    "\t# Flatten the data\n",
    "\t\n",
    "\t# <END>\n",
    "\treturn x_train, x_test\n",
    "\n",
    "x_train, x_test = preprocess(x_train, x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot encoding is a method of representing the labels. You can read about it here: https://www.geeksforgeeks.org/ml-one-hot-encoding/ . We will convert y_train and y_test to numpy arrays and then one-hot encode them + reshape them to a matrix instead of a vector.\n",
    "\n",
    "Make sure that you appreciate the reason for one-hot encoding the labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hint:\n",
    "<details> You can use the function np.eye() for one-hot encoding </details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(y_train, y_test):\n",
    "\t# <Start>\n",
    "\t# Convert to numpy arrays\n",
    "\n",
    "\t# Apply one-hot encoding\n",
    "\t\n",
    "\t# Reshape the data\n",
    "\t\n",
    "\t# <End>\n",
    "\treturn y_train, y_test\n",
    "\n",
    "y_train, y_test = one_hot(y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will make a neural network with 4 layers of neurons (1 input layer, 2 hidden layers, 1 output layer). The size of the layers (number of neurons in them) would be (784, 128, 32, __). Guess the final layers size and make a tuple n with these values.\n",
    "\n",
    "Note that the choice of neurons in the hidden layers i.e. 128 and 32 in hidden layer 1 and 2 respectively is subjective here. However, you would always what to decrease the number of neurons in subsequent layers (make sure you understand why?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = (None, None, None, None) # Replace None with the correct values \n",
    "epochs = None # Replace None with the correct values. Keep the epochs low (1-3) while testing the code. Once you are confident that the code is working fine, you can set the epochs to 10-20. This is just to save time.\n",
    "lr = None # This is the learning rate. Keep it in the range of 0.001 to 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class NN, short for neural network :/, is made in the next block. The train function already completed. First look at the training loop to understand the flow and inputs to various functions in this neural network. Then complete the functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "class NN:\n",
    "\tdef __init__(self, n, epochs, lr):\n",
    "\t\tself.n = n # tuple specifing the number of neurons in each layer the 2 hidden layer arcitecture.\n",
    "\t\tself.epochs = epochs\n",
    "\t\tself.lr = lr\n",
    "\n",
    "\t\tself.input_layer = n[0]\n",
    "\t\tself.hidden_layer1 = n[1]\n",
    "\t\tself.hidden_layer2 = n[2]\n",
    "\t\tself.output_layer = n[3]\n",
    "\n",
    "\t\t# Make a dictionary self.params to store the weights and biases \n",
    "\t\t# The keys will be W1, W2, W3, b1, b2, b3\n",
    "\t\t# Figure out the shapes of these weights and biases vectors\n",
    "\t\t# Initialize the weights and biases with random values (using a suitable numpy function ofcourse)\n",
    "\t\tself.params = { \n",
    "\t\t\t# <START>\n",
    "\t\t\t'W1':None,\n",
    "\t\t\t'W2':None,\n",
    "\t\t\t'W3':None,\n",
    "\t\t\t'b1':None, \n",
    "\t\t\t'b2':None,\n",
    "\t\t\t'b3':None\n",
    "\t\t\t# <END>\n",
    "\t\t}\n",
    "\n",
    "\t# In a neural network, we need an activation function.\n",
    "\t# You can read about activation functions here: https://www.geeksforgeeks.org/activation-functions-neural-networks/\n",
    "\t# Here, we will use the sigmoid function\n",
    "\tdef sigmoid(self, x, derivative=False):\n",
    "\t\t\"\"\"\n",
    "\t\tThis function returns \n",
    "\t\t\t> sigmoid(x) when derivative is false\n",
    "\t\t\t> sigmoid'(x) when derivative is trye\n",
    " \t\t\"\"\"\n",
    "\t\tif (derivative):\n",
    "\t\t\treturn x # Dummy return\n",
    "\t\treturn # Dummy return\n",
    "\t\n",
    "\t# We will also use the softmax function (we will use this in our final layer)\n",
    "\t# You can read about the softmax function here: https://www.geeksforgeeks.org/the-role-of-softmax-in-neural-networks-detailed-explanation-and-applications/\n",
    "\tdef softmax(self, x, derivative=False):\n",
    "\t\te_x = np.exp(x - np.max(x)) # Why x - np.max(x)? The values in the array e_x will now be in the range 0 to 1.\n",
    "\t\tif derivative:\n",
    "\t\t\treturn e_x # Dummy return\n",
    "\t\treturn e_x # Dummy return\n",
    "\n",
    "\t# Feed forward the image through the network i.e. do forward propogation\n",
    "\tdef feed_forward(self, image):\n",
    "\t\tparams = self.params\n",
    "\n",
    "\t\tparams['a0'] = image\n",
    "\n",
    "\t\t# <START>\n",
    "\t\tparams['z1'] = None # z1 = W1@image + b1\n",
    "\t\tparams['a1'] = None # Applly the sigmoid activation function\n",
    "\n",
    "\t\tparams['z2'] = None # z2 = W2@image + b2\n",
    "\t\tparams['a2'] = None # Apply the sigmoid activation function \n",
    "\t\t\n",
    "\t\tparams['z3'] = None # z3 = W3@image + b3\n",
    "\t\tparams['a3'] = None # Apply sofmax activation function (since this is the final layer)\n",
    "\t\t#<END>\n",
    "\n",
    "\t\treturn params['a3']\n",
    "\n",
    "\t# Loss for the prediction of label for one image\n",
    "\t# Use Mean Squared Error loss\n",
    "\tdef loss(self, output_label, true_label):\n",
    "\t\treturn 0 # Dummy return\n",
    "\t\n",
    "\t# Back propagation is the process of updating the weights of the neural network to minimize the loss (the very step that makes the neural network learn)\n",
    "\t# You can read about back propagation here: https://www.geeksforgeeks.org/backpropagation-in-neural-network/\n",
    "\tdef back_propagation(self, output, y_true):\n",
    "\t\tparams = self.params\n",
    "\n",
    "\t\tchange_w = {}\n",
    "\t\tchange_b = {}\n",
    "\n",
    "\t\t# <START> This one is going to be difficult unless you know the maths.\n",
    "\t\tchange_w['W3'] = None # Dummy value\n",
    "\t\tchange_b['b3'] = None # Dummy value\n",
    "\n",
    "\t\tchange_w['W2'] = None # Dummy value\n",
    "\t\tchange_b['b2'] = None # Dummy value\n",
    "\n",
    "\t\tchange_w['W1'] = None # Dummy value\n",
    "\t\tchange_b['b1'] = None # Dummy value\n",
    "\t\t# <END>\n",
    "\n",
    "\t\treturn change_w, change_b\n",
    "\t\n",
    "\t# We just recorded what the change in weights and biases should be. Now, we will actually update the weights and biases (using Gradient Descent)\n",
    "\tdef update_weights(self, change_w, change_b):\n",
    "\t\tparams = self.params\n",
    "\t\t# <START>\n",
    "\t\tparams['W3'] = None\n",
    "\t\tparams['b3'] = None\n",
    "\n",
    "\t\tparams['W2'] = None\n",
    "\t\tparams['b2'] = None\n",
    "\n",
    "\t\tparams['W1'] = None\n",
    "\t\tparams['b1'] = None\n",
    "\t\t# <END>\n",
    "\n",
    "\t# Training the neural network\n",
    "\t# We will feed forward the image, back propagate and update the weights and biases. This will be done for each image for the number of epochs specified.\n",
    "\t# The only place where you can use for loops\n",
    "\tdef train(self, x_train, y_train) -> None:\n",
    "\t\tfor i in range(self.epochs):\n",
    "\t\t\tfor image, label in tqdm(zip(x_train, y_train), total=len(x_train), desc=f'Epoch {i+1}/{self.epochs}'):\n",
    "\t\t\t\tpred = self.feed_forward(image)\n",
    "\t\t\t\tchange_w, change_b = self.back_propagation(pred, label)\n",
    "\t\t\t\tself.update_weights(change_w, change_b)\n",
    "\n",
    "\t# Predict the label for each image and store them in an array. This array will be used to calculate the accuracy of the model\n",
    "\t# You can use for loops here too :P\n",
    "\tdef predict(self, x_test):\n",
    "\t\tpredictions = np.array([])\n",
    "\t\tfor image in x_test:\n",
    "\t\t\tpred = self.feed_forward(image)\n",
    "\t\t\tpred = np.argmax(pred)\n",
    "\t\t\tpredictions = np.append(predictions, pred)\n",
    "\n",
    "\t\treturn predictions\n",
    "\t\n",
    "\t# What's accuracy? Number of correct predictions / total number of predictions\n",
    "\tdef accuracy(self, y_test, y_pred):\n",
    "\t\tcount = 0\n",
    "\t\ty_true = y_test.reshape(y_test.shape[0], -1)\n",
    "\t\ty_true = np.argmax(y_true, axis=1)\n",
    "\t\tfor i in range(len(y_true)):\n",
    "\t\t\tif y_true[i] == y_pred[i]:\n",
    "\t\t\t\tcount += 1\n",
    "\t\treturn count / len(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you point out the limitations of the above implementation? \n",
    "<details> It only works for a 2 hidden layer neural network </details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task:\n",
    "Play around with the number of the number of neurons in the hidden layer, number of epochs and the learning rate and observe how these HYPERPARAMETERS affect the outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NN(n, epochs, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.train(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = nn.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nn.accuracy(y_test, y_pred)*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
